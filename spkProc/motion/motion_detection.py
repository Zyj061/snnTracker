from utils import get_kernel, get_transform_matrix_new, visualize_images
# import kornia as tgm
import numpy as np
import torch.nn.functional as F
import torch
import os

class motion_estimation:

    def __init__(self, dvs_h, dvs_w, device, logger, debug_mode=False, debug_frame_target=200, debug_require_nonzero=True, debug_min_spike_ratio=0.0):

        self.dvs_h = dvs_h
        self.dvs_w = dvs_w
        self.device = device
        self.logger = logger

        # motion parameters
        self.orientation = range(0, 180 - 1, int(180 / 4))
        # eight moving direction
        '''
                self.ori = torch.Tensor(np.array([[-1, -1],
                    [0, -1],
                    [1, -1],
                    [-1, 0],
                    [1, 0],
                    [-1, 1],
                    [0, 1],
                    [1, 1]], dtype=np.uint8)).to(self.device)
        '''
        
        self.ori = np.array([[1, 0],
                             [1, 1],
                             [0, 1],
                             [-1, 1],
                             [-1, 0],
                             [-1, -1],
                             [0, -1],
                             [1, -1]], dtype=np.int32)
        self.speed = np.array([1, 2], dtype=np.int32)
        self.ori_x = torch.from_numpy(np.expand_dims(self.ori[:, 0], axis=1)).to(self.device).float()
        self.ori_y = torch.from_numpy(np.expand_dims(self.ori[:, 1], axis=1)).to(self.device).float()

        self.warp_matrix = get_transform_matrix_new(self.ori, self.speed, self.dvs_w, self.dvs_h, self.device)
        self.track_pre = torch.zeros(self.dvs_h, self.dvs_w)

        self.num_ori = len(self.ori)
        self.num_speed = len(self.speed)
        self.motion_pattern_num = self.num_ori * self.num_speed
        self.motion_weight = torch.ones(self.motion_pattern_num, 1, self.dvs_h, self.dvs_w) / self.motion_pattern_num
        self.tracking_threshold = 1

        self.local_pool_size = 11
        padding_width = int((self.local_pool_size - 1) / 2)
        self.pool_kernel = torch.nn.Conv2d(in_channels=1, out_channels=1,
                                           kernel_size=(self.local_pool_size, self.local_pool_size),
                                           padding=(padding_width, padding_width), bias=False)
        self.pool_kernel.weight.data = torch.ones(1, 1, self.local_pool_size, self.local_pool_size)

        self.gaussian_kernel = torch.nn.Conv2d(in_channels=1, out_channels=1,
                                               kernel_size=(self.local_pool_size, self.local_pool_size),
                                               padding=(padding_width, padding_width), bias=False)
        tmp_filter = get_kernel(self.local_pool_size, round(self.local_pool_size / 4))
        tmp_filter = tmp_filter.reshape((1, 1, self.local_pool_size, self.local_pool_size))
        self.gaussian_kernel.weight.data = torch.from_numpy(tmp_filter).float()

        # local wta inhibition size
        self.inh_size = 25
        self.padding_width = int((self.inh_size - 1) / 2)
        self.inhb_kernel = torch.nn.Conv2d(in_channels=1, out_channels=1,
                                           kernel_size=(self.inh_size, self.inh_size),
                                           padding=(self.padding_width, self.padding_width), bias=False)
        self.inhb_kernel.weight.data = torch.ones(1, 1, self.inh_size, self.inh_size)
        self.inhb_threshold = 5

        self.track_pre = self.track_pre.to(self.device)
        self.motion_weight = self.motion_weight.to(self.device)
        self.pool_kernel = self.pool_kernel.to(self.device)
        self.gaussian_kernel = self.gaussian_kernel.to(self.device)
        self.inhb_kernel = self.inhb_kernel.to(self.device)

        self._grid_cache = {}
        # Preallocate reusable buffer to reduce per-call allocations in local_wta
        self._track_voltage = torch.empty(self.num_ori, self.dvs_h, self.dvs_w, device=self.device)

        cc_motion = [[0, 255, 255],
                     [205, 95, 85],
                     [11, 134, 184],
                     [255, 255, 0],
                     [154, 250, 0],
                     [147, 20, 255],
                     [240, 32, 160],
                     [48, 48, 255]]

        cc_motion = np.transpose(np.array(cc_motion, dtype=np.float32))
        self.cc_motion = torch.from_numpy(cc_motion / 255)
        self.cc_motion = self.cc_motion.to(self.device)
        self.learning_rate = 0.1

        # Cache direction indices tensor to avoid per-call allocations in local_wta
        self._direction_indices = torch.arange(self.num_ori, device=self.device).view(-1, 1, 1)

        # Spatial downsample factors for stdp_tracking (env-controlled)
        # Set STDP_DOWNSAMPLE_H/W (>1) to enable. Example: export STDP_DOWNSAMPLE_H=2; export STDP_DOWNSAMPLE_W=2
        try:
            self.ds_h = int(os.environ.get('STDP_DOWNSAMPLE_H', '1'))
        except Exception:
            self.ds_h = 1
        try:
            self.ds_w = int(os.environ.get('STDP_DOWNSAMPLE_W', '1'))
        except Exception:
            self.ds_w = 1

        # Optional: periodically call torch.cuda.empty_cache every N calls to local_wta (0 disables)
        try:
            self.empty_cache_every = int(os.environ.get('EMPTY_CACHE_EVERY', '0'))
        except Exception:
            self.empty_cache_every = 0
        self._call_step = 0

        '''
        self.dw_ltp = torch.zeros(self.motion_pattern_num, 1, self.dvs_h, self.dvs_w)
        self.dw_ltd = torch.zeros(self.motion_pattern_num, 1, self.dvs_h, self.dvs_w)
        self.dw_ltp = self.dw_ltp.to(self.device)
        self.dw_ltd = self.dw_ltd.to(self.device)       
        '''

    def stdp_tracking(self, spikes):
        """
        å‚æ•°:
            spikes: torch.Tensor
                - å½¢çŠ¶: (H, W)
                - ç±»åž‹: torch.float32 æˆ– torch.uint8
                - å«ä¹‰: å½“å‰å¸§çš„è„‰å†²äº‹ä»¶çŸ©é˜µï¼ŒHä¸ºé«˜åº¦ï¼ŒWä¸ºå®½åº¦ã€‚æ¯ä¸ªå…ƒç´ ä¸º0æˆ–1ï¼Œ1è¡¨ç¤ºè¯¥åƒç´ åœ¨å½“å‰å¸§æœ‰è„‰å†²äº‹ä»¶å‘ç”Ÿï¼Œ0è¡¨ç¤ºæ— äº‹ä»¶ã€‚
        """
        
        prev_grad_enabled = torch.is_grad_enabled()
        torch.set_grad_enabled(False)
        # # Compute on downsampled grid to reduce cost, then upsample results back to original size
        if not torch.is_floating_point(spikes):
            spikes = spikes.float()
        use_ds = hasattr(self, 'ds_h') and hasattr(self, 'ds_w') and (self.ds_h > 1 or self.ds_w > 1)
        H_orig, W_orig = int(self.dvs_h), int(self.dvs_w)
        if use_ds:
            kh, kw = int(self.ds_h), int(self.ds_w)
            spikes_4d = torch.reshape(spikes, (1, 1, H_orig, W_orig))
            out_h = (H_orig + kh - 1) // kh
            out_w = (W_orig + kw - 1) // kw
            pad_h = out_h * kh - H_orig
            pad_w = out_w * kw - W_orig
            if pad_h > 0 or pad_w > 0:
                spikes_4d = F.pad(spikes_4d, (0, pad_w, 0, pad_h), mode='constant', value=0)
            spikes_ds_4d = F.max_pool2d(spikes_4d, kernel_size=(kh, kw), stride=(kh, kw))
            track_post = spikes_ds_4d  # 1 x 1 x H_ds x W_ds
            H2, W2 = int(track_post.shape[2]), int(track_post.shape[3])
            spikes_used_2d = torch.squeeze(track_post)
            # Downsample previous track_pre accordingly for fair comparison
            tp_4d = torch.reshape(self.track_pre, (1, 1, H_orig, W_orig)).to(track_post.device)
            if pad_h > 0 or pad_w > 0:
                tp_4d = F.pad(tp_4d, (0, pad_w, 0, pad_h), mode='constant', value=0)
            track_pre_ds_4d = F.max_pool2d(tp_4d, kernel_size=(kh, kw), stride=(kh, kw))
            track_pre_2d = torch.squeeze(track_pre_ds_4d)
        else:
            track_post = torch.reshape(spikes, (1, 1, H_orig, W_orig))
            H2, W2 = H_orig, W_orig
            spikes_used_2d = torch.squeeze(track_post)
            track_pre_2d = self.track_pre
        tmp_pool = self.pool_kernel(track_post)
        
        # Use expand to avoid real memory replication
        tmp_pool = tmp_pool.expand(self.motion_pattern_num, 1, tmp_pool.shape[-2], tmp_pool.shape[-1])

        # predict_firedå˜é‡ç”¨äºŽå­˜å‚¨åœ¨æ‰€æœ‰è¿åŠ¨æ¨¡å¼ï¼ˆæ–¹å‘Ã—é€Ÿåº¦ï¼‰ä¸‹ï¼Œå½“å‰å¸§æ‰€æœ‰æ¿€æ´»åƒç´ ç‚¹åœ¨ä¸‹ä¸€ä¸ªæ—¶åˆ»çš„é¢„æµ‹è„‰å†²ä½ç½®ã€‚
        # ä½¿ç”¨æ‰¹é‡ä»¿å°„ç½‘æ ¼é‡‡æ ·ï¼Œä¸€æ¬¡æ€§å¯¹æ‰€æœ‰è¿åŠ¨æ¨¡å¼è¿›è¡Œä½ç§»é¢„æµ‹ï¼Œå……åˆ†åˆ©ç”¨GPUå¹¶è¡Œã€‚
        # æž„é€ ä¸Žå½“å‰åˆ†è¾¨çŽ‡åŒ¹é…çš„ä»¿å°„çŸ©é˜µ
        cache_key = (int(H2), int(W2))
        if cache_key in self._grid_cache:
            warp_theta, grid = self._grid_cache[cache_key]
        else:
            if use_ds and (H2 != self.dvs_h or W2 != self.dvs_w):
                warp_theta = get_transform_matrix_new(self.ori, self.speed, W2, H2, self.device)
            else:
                warp_theta = self.warp_matrix
            grid = F.affine_grid(warp_theta, torch.Size([self.motion_pattern_num, 1, H2, W2]), align_corners=True)
            self._grid_cache[cache_key] = (warp_theta, grid)
        # æ‰¹é‡å¤åˆ¶è¾“å…¥åˆ° motion_pattern_num ä¸ªé€šé“ï¼Œä½¿ç”¨ expand é¿å…çœŸå®žæ‹·è´
        track_post_batched = track_post.expand(self.motion_pattern_num, -1, -1, -1)
        predict_fired = F.grid_sample(track_post_batched, grid, mode='nearest', padding_mode='zeros', align_corners=True)
        # äºŒå€¼åŒ–
        predict_fired = (predict_fired > 0).to(track_post.dtype)        

        # unsqueezeå‡½æ•°çš„ä½œç”¨æ˜¯ç»™self.track_preè¿™ä¸ªå¼ é‡åœ¨ç¬¬0ç»´å¢žåŠ ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œ
        # è¿™æ ·åŽŸæœ¬å½¢çŠ¶ä¸º (1, H, W) çš„å¼ é‡å˜æˆ (1, 1, H, W)ã€‚
        # è¿™é‡Œçš„repeat(self.motion_pattern_num, 1, 1) æ˜¯æŠŠç¬¬0ç»´ï¼ˆå³motion patternçš„æ•°é‡ï¼‰æ‰©å±•æˆself.motion_pattern_numä»½ï¼Œ
        # ç»“æžœtrack_pre_expçš„å½¢çŠ¶å˜ä¸º (self.motion_pattern_num, 1, H, W)ï¼Œ
        # è¿™æ ·åŽç»­å¯ä»¥å’Œæ¯ä¸ªè¿åŠ¨æ¨¡å¼çš„é¢„æµ‹è„‰å†²ä½ç½®è¿›è¡Œé€å…ƒç´ æ¯”è¾ƒã€‚
        track_pre_exp = track_pre_2d.unsqueeze(0).unsqueeze(1).expand(self.motion_pattern_num, 1, H2, W2)

        # STDP update the motion weight (vectorized, avoiding sparse index writes)
        tmp_bool = torch.eq(predict_fired, track_pre_exp)
        # LTP: predict == track_pre == 1
        dw_ltp = (torch.logical_and(tmp_bool, track_pre_exp == 1)).to(track_post.dtype)
        # LTD: predict == 1 and predict != track_pre
        dw_ltd = (torch.logical_and(~tmp_bool, predict_fired == 1)).to(track_post.dtype) * 2

        # Debug checkpoint: pre-pooling dw_ltp/dw_ltd stats (only during save_output_data replay)
        
        dw_ltp = self.pool_kernel(dw_ltp)
        dw_ltd = self.pool_kernel(dw_ltd)

        # dw_ltp = self.gaussian_kernel(dw_ltp)
        # dw_ltd = self.gaussian_kernel(dw_ltd)

        # dw = dw_ltp - dw_ltd
        # dw = self.gaussian_kernel(dw_ltp - dw_ltd)
        # tmp_pool[torch.where(tmp_pool == 0)] = 1
        numerator = (dw_ltp - dw_ltd)
        safe_denom = torch.where(tmp_pool != 0, tmp_pool, torch.ones_like(tmp_pool))
        quotient = numerator / safe_denom
        dw = torch.where(tmp_pool != 0, quotient, torch.zeros_like(quotient))

        # If computed on downsampled grid, upsample dw to original size before applying to motion_weight
        if use_ds:
            dw = F.interpolate(dw, size=(H_orig, W_orig), mode='nearest')
        # dw = dw / tmp_pool
        # dw = dw_ltp - dw_ltd
        with torch.no_grad():
            self.motion_weight += self.learning_rate * dw

        max_weight, _ = torch.max(self.motion_weight, dim=0)
        min_weight, _ = torch.min(self.motion_weight, dim=0)

        # Normalization
        denom = (max_weight - min_weight)
        valid_mask = denom > 0
        # Vectorized normalization across motion dimension
        self.motion_weight = torch.where(
            valid_mask,
            (self.motion_weight - min_weight) / denom,
            self.motion_weight
        )
        
        # self.motion_weight.data = F.normalize(self.motion_weight, p=2, dim=0)
        if torch.isnan(self.motion_weight).any():
            raise AssertionError("NaN detected in motion_weight")
        # self.motion_weight[torch.isinf(self.motion_weight)] = 0
        if use_ds:
            # Upsample downsampled spikes back to original size to keep track_pre shape and semantics consistent
            spikes_us = F.interpolate(track_post, size=(H_orig, W_orig), mode='nearest')
            with torch.no_grad():
                self.track_pre = torch.squeeze(spikes_us)
        else:
            with torch.no_grad():
                self.track_pre = spikes

        # del track_post, tmp_pool, predict_fired, track_pre_exp, tmp_bool, dw
        # del tmp_weight, max_weight, min_weight, spikes
        # del dw_ltd, dw_ltp
        # torch.cuda.empty_cache()

        # restore global grad setting
        torch.set_grad_enabled(prev_grad_enabled)

    def compute_motion_direction_gpu(self, dx, dy):
        """
        åœ¨GPUä¸Šç›´æŽ¥è®¡ç®—è¿åŠ¨æ–¹å‘ï¼Œé¿å…CPU-GPUæ•°æ®ä¼ è¾“
        æ›¿ä»£åŽŸæ¥çš„numpy.arctan2è®¡ç®—ï¼Œæ¶ˆé™¤æ€§èƒ½ç“¶é¢ˆ
        
        Args:
            dx: xæ–¹å‘è¿åŠ¨åˆ†é‡ (torch.Tensor on GPU)
            dy: yæ–¹å‘è¿åŠ¨åˆ†é‡ (torch.Tensor on GPU)
            
        Returns:
            tmp_motion: æ–¹å‘ç¼–å· (0-7) (torch.Tensor on GPU)
        """
        # ä½¿ç”¨ torch.atan2 æ›¿ä»£ numpy.arctan2ï¼Œä¿æŒåœ¨GPUä¸Šè®¡ç®—
        rotAng = torch.atan2(-dy, dx) * 180 / torch.pi + 180
        
        # å¤„ç†è¾¹ç•Œæ¡ä»¶ï¼šå°†360åº¦çš„è§’åº¦å½’é›¶ï¼Œä¿è¯èŒƒå›´åœ¨[0,360)
        rotAng = torch.where(rotAng >= 360, rotAng - 360, rotAng)
        
        # å°†è§’åº¦å‡åˆ†ä¸º8ä¸ªæ–¹å‘åŒºé—´ï¼ˆæ¯ä¸ªåŒºé—´45åº¦ï¼‰ï¼Œå¾—åˆ°æ–¹å‘ç¼–å·ï¼ˆ0~7ï¼‰
        tmp_motion = torch.floor(rotAng / (360 / 8)).long()
        
        # ç¡®ä¿æ–¹å‘ç¼–å·åœ¨æœ‰æ•ˆèŒƒå›´å†… (0-7)
        tmp_motion = torch.clamp(tmp_motion, 0, 7)
        
        return tmp_motion

    def local_wta(self, spikes, timestamp, visualize=False):
        # Ensure spikes are on the correct device without unnecessary transfers
        if spikes.device != self.device:
            spikes = spikes.to(self.device, non_blocking=True)
        input_spike = torch.reshape(spikes, (1, 1, self.dvs_h, self.dvs_w))

        if False:
            # æ‰“å°self.deviceæ˜¯CPUè¿˜æ˜¯GPU
            print(f"å½“å‰è®¾å¤‡ä¸º: {self.device} ({'GPU' if 'cuda' in str(self.device) and torch.cuda.is_available() else 'CPU'})")
            # åˆ¤æ–­ self.motion_weight å½“å‰æ˜¯åœ¨CPUè¿˜æ˜¯GPUä¸Š
            print(f"self.motion_weight å½“å‰æ‰€åœ¨è®¾å¤‡: { 'GPU' if 'cuda' in str(self.motion_weight.device) and torch.cuda.is_available() else 'CPU'}")
            print(f"spikes             å½“å‰æ‰€åœ¨è®¾å¤‡: { 'GPU' if 'cuda' in str(spikes.device) and torch.cuda.is_available() else 'CPU'}")
        
        
        motion_vector_layer1 = torch.zeros(self.dvs_h, self.dvs_w, 2, dtype=torch.float32, device=self.device)
        max_w, max_wid = torch.max(self.motion_weight, dim=0)
        max_wid = torch.squeeze(max_wid)
        speedId = (max_wid % self.num_speed).detach()
        oriId = (torch.floor(max_wid / self.num_speed)).detach()

        # è¿™é‡Œå°†motion_weightçš„ç»´åº¦ä»Ž (motion_pattern_num, 1, dvs_h, dvs_w)
        # å˜æ¢ä¸º (dvs_h, dvs_w, 1, motion_pattern_num)ï¼Œ
        # æ–¹ä¾¿åŽç»­reshapeå’Œä¸Žæ–¹å‘ã€é€Ÿåº¦å‘é‡çš„çŸ©é˜µè¿ç®—ã€‚
        tmp_weight = self.motion_weight.permute(2, 3, 1, 0)
        # change the dimension of matrix from (ori_num, speed_num, height, width) to (h,w, speed_num, ori_num)
        tmp_weight = torch.reshape(tmp_weight, [self.dvs_h, self.dvs_w, self.num_ori, self.num_speed])
        tmp_weight = tmp_weight.permute(0, 1, 3, 2)

        # è¿™ä¸¤ä¸ªçŸ©é˜µä¹˜æ³•çš„æ„ä¹‰ï¼š
        # tmp_weightçš„å½¢çŠ¶ä¸º (dvs_h, dvs_w, num_speed, num_ori)
        # self.ori_x å’Œ self.ori_y åˆ†åˆ«æ˜¯æ–¹å‘çš„xåˆ†é‡å’Œyåˆ†é‡ï¼Œå½¢çŠ¶ä¸º (num_ori, 1)
        # ä¸‹é¢çš„matmulæ“ä½œç›¸å½“äºŽå¯¹æ¯ä¸ªåƒç´ ã€æ¯ä¸ªé€Ÿåº¦ï¼Œå°†æ‰€æœ‰æ–¹å‘çš„æƒé‡åŠ æƒåˆ°x/yåˆ†é‡ä¸Šï¼Œå®žçŽ°æ–¹å‘åˆ†è§£
        # ç»“æžœtmp_weight_x, tmp_weight_yå½¢çŠ¶ä¸º (dvs_h, dvs_w, num_speed, 1)
        tmp_weight_x = torch.matmul(tmp_weight, self.ori_x)   # æŠŠxæ–¹å‘è¿™ä¸€ä¸ªç»´åº¦ï¼Œé€šè¿‡çº¿æ€§ç»„åˆï¼Œåˆå¹¶æŽ‰äº†
        tmp_weight_y = torch.matmul(tmp_weight, self.ori_y)   # æŠŠyæ–¹å‘è¿™ä¸€ä¸ªç»´åº¦ï¼Œé€šè¿‡çº¿æ€§ç»„åˆï¼Œåˆå¹¶æŽ‰äº†
        # tmp_weight_x = torch.reshape(torch.mm(tmp_weight, self.ori_x), [self.dvs_h, self.dvs_w, self.num_speed])
        # tmp_weight_y = torch.reshape(torch.mm(tmp_weight, self.ori_y), [self.dvs_h, self.dvs_w, self.num_speed])

        max_w = torch.squeeze(max_w)
        fired_spk = torch.logical_and(spikes != 0, max_w > 0)

        tmp_weight_x = torch.mean(tmp_weight_x, dim=2)
        tmp_weight_y = torch.mean(tmp_weight_y, dim=2)
        tmp_weight_x = torch.squeeze(tmp_weight_x)
        tmp_weight_y = torch.squeeze(tmp_weight_y)

        # ðŸš€ GPUä¼˜åŒ–ï¼šä½¿ç”¨ç¨ å¯†è®¿é—®æ›¿ä»£ç¨€ç–é—´æŽ¥è®¿é—®ï¼Œæå‡GPUå¹¶è¡Œæ€§èƒ½
        # è®¡ç®—æ¯ä¸ªæ¿€æ´»åƒç´ ç‚¹çš„x/yæ–¹å‘è¿åŠ¨åˆ†é‡ï¼Œå¹¶å†™å…¥motion_vector_layer1
        # ä½¿ç”¨ç¨ å¯†çŸ©é˜µä¹˜æ³•æ›¿ä»£ç¨€ç–ç´¢å¼•æ“ä½œ
        motion_vector_layer1[:, :, 0] = tmp_weight_x * fired_spk.float()
        motion_vector_layer1[:, :, 1] = tmp_weight_y * fired_spk.float()
        
        # æå–æ¿€æ´»åƒç´ çš„è¿åŠ¨åˆ†é‡ä¾›åŽç»­è®¡ç®—ä½¿ç”¨
        dx = tmp_weight_x[fired_spk]
        dy = tmp_weight_y[fired_spk]
        

        # ï¿½ï¿½ GPUä¼˜åŒ–ï¼šä½¿ç”¨GPUä¸Šçš„è§’åº¦è®¡ç®—ï¼Œæ¶ˆé™¤CPU-GPUæ•°æ®ä¼ è¾“ç“¶é¢ˆ
        # å¯¹æ‰€æœ‰åƒç´ è¿›è¡Œç¨ å¯†è®¡ç®—ï¼Œè€Œä¸ä»…ä»…æ˜¯æ¿€æ´»åƒç´ 
        tmp_motion_full = self.compute_motion_direction_gpu(tmp_weight_x, tmp_weight_y)
        
        # ä¸ºå‘åŽå…¼å®¹ï¼Œä¿ç•™æ¿€æ´»åƒç´ çš„æ–¹å‘æ•°æ®
        tmp_motion = self.compute_motion_direction_gpu(dx, dy)
        
        # ðŸš€ GPUä¼˜åŒ–ï¼šæž„å»ºæ–¹å‘-ç©ºé—´ä¸‰ç»´ç”µåŽ‹å›¾ï¼ˆtrack_voltageï¼‰ï¼Œä½¿ç”¨ç¨ å¯†è®¿é—®
        #    å½¢çŠ¶ä¸º[num_ori, dvs_h, dvs_w]ï¼Œæ¯ä¸ªåƒç´ ç‚¹åœ¨å…¶å¯¹åº”æ–¹å‘é€šé“ç½®1ï¼Œå…¶ä½™ä¸º0
        track_voltage = self._track_voltage
        track_voltage.zero_()
        # å°† fired_spk å¯¹åº”åƒç´ æŒ‰æ–¹å‘å¡« 1ï¼ˆscatter æ–¹å¼ï¼‰ï¼š
        # ç´¢å¼•å½¢çŠ¶: (1, H, W) çš„æ–¹å‘ç´¢å¼•
        tmp_motion_expanded = tmp_motion_full.view(1, self.dvs_h, self.dvs_w)
        fired_spk_float = fired_spk.float()
        # åœ¨å¯¹åº”æ–¹å‘é€šé“ä½ç½®åŠ ä¸Š maskï¼ˆå€¼ä¸º1ï¼‰ï¼›å…¶ä»–ä¸º0
        track_voltage.scatter_(0, tmp_motion_expanded, fired_spk_float.unsqueeze(0))

        # 5. å·ç§¯æŠ‘åˆ¶æ“ä½œ
        #    - å…ˆåœ¨æ–¹å‘ç»´åº¦æ’å…¥ä¸€ç»´ï¼ˆå˜æˆ[æ–¹å‘,1,H,W]ï¼‰ï¼Œä»¥é€‚é…å·ç§¯æ ¸è¾“å…¥
        #    - ç§»åŠ¨åˆ°è®¾å¤‡ä¸Šï¼ˆå¦‚GPUï¼‰
        #    - é€šè¿‡self.inhb_kernelè¿›è¡Œå±€éƒ¨æŠ‘åˆ¶ï¼ˆå¦‚Winner-Take-Allï¼‰ï¼Œå†squeezeåŽ»æŽ‰å¤šä½™ç»´åº¦
        track_voltage = torch.unsqueeze(track_voltage, 1)
        track_voltage = torch.squeeze(self.inhb_kernel(track_voltage))
        # 6. å¯¹æ¯ä¸ªåƒç´ ç‚¹ï¼Œæ‰¾åˆ°æ–¹å‘é€šé“ä¸­æœ€å¤§ç”µåŽ‹å€¼åŠå…¶æ–¹å‘ç¼–å·
        max_v, max_vid = torch.max(track_voltage, dim=0)

        # 7. ç¬¬äºŒå±‚æ¿€æ´»åƒç´ ç­›é€‰
        #    - åªæœ‰æ»¡è¶³ï¼šæœ€å¤§ç”µåŽ‹å¤§äºŽé˜ˆå€¼ã€åŽŸå§‹spikeæ¿€æ´»ã€æœ€å¤§æƒé‡å¤§äºŽ0 çš„åƒç´ ç‚¹æ‰è¢«è®¤ä¸ºæ˜¯æœ‰æ•ˆè¿åŠ¨ç‚¹
        fired_layer2_mask = torch.logical_and(max_v >= self.inhb_threshold, torch.logical_and(spikes != 0, max_w > 0))
        # 8. åˆå§‹åŒ–æœ€å¤§æ–¹å‘ç¼–å·å›¾ï¼ˆmax_motionï¼‰ã€ç¬¬ä¸€å±‚æ–¹å‘ç¼–å·å›¾ï¼ˆmax_motion_layer1ï¼‰ã€æœ€å¤§è¿åŠ¨çŸ¢é‡å›¾ï¼ˆmotion_vector_maxï¼‰
        max_motion = torch.zeros(self.dvs_h, self.dvs_w, dtype=torch.int64, device=self.device)
        max_motion_layer1 = torch.zeros(self.dvs_h, self.dvs_w, dtype=torch.int64, device=self.device)

        motion_vector_max = torch.zeros(self.dvs_h, self.dvs_w, 2, dtype=torch.float32, device=self.device)

        # 9. å¯¹äºŽç¬¬äºŒå±‚æ¿€æ´»åƒç´ ç‚¹ï¼Œè®°å½•å…¶æœ€å¤§æ–¹å‘ç¼–å·ï¼ˆmax_vid+1ï¼Œæ–¹å‘ç¼–å·ä»Ž1å¼€å§‹ï¼‰
        max_motion[fired_layer2_mask] = max_vid[fired_layer2_mask].detach() + 1
        # 10. å¯¹äºŽç¬¬ä¸€å±‚æ¿€æ´»åƒç´ ç‚¹ï¼Œè®°å½•å…¶æ–¹å‘ç¼–å·ï¼ˆtmp_motion+1ï¼Œæ–¹å‘ç¼–å·ä»Ž1å¼€å§‹ï¼‰
        # ðŸš€ GPUä¼˜åŒ–ï¼štmp_motionå·²ç»æ˜¯GPU tensorï¼Œæ— éœ€è½¬æ¢
        max_motion_layer1[fired_spk] = (tmp_motion + 1).long()
        
        # ðŸš€ GPUä¼˜åŒ–ï¼šå¯¹äºŽç¬¬ä¸€å±‚æ¿€æ´»åƒç´ ç‚¹ï¼Œä½¿ç”¨ç¨ å¯†è®¿é—®è®°å½•æ–¹å‘ç¼–å·ï¼ˆä»Ž1å¼€å§‹ï¼‰
        # ä½¿ç”¨ç¨ å¯†çŸ©é˜µæ“ä½œæ›¿ä»£ç¨€ç–ç´¢å¼•ï¼Œtmp_motion_fullå·²ç»æ˜¯GPU tensor
        # max_motion_layer1 = torch.where(fired_spk, (tmp_motion_full + 1).long(), max_motion_layer1)

        # 11. å¯¹äºŽæœªè¢«ç¬¬äºŒå±‚æ¿€æ´»çš„åƒç´ ç‚¹ï¼Œå°†å…¶ç¬¬ä¸€å±‚æ–¹å‘ç¼–å·æ¸…é›¶
        max_motion_layer1[max_motion == 0] = 0

        # 1. find the difference between m1 and mc motion
        has_layer2 = fired_layer2_mask.any()
        if has_layer2:
            tmp_vid_tensor = max_vid[fired_layer2_mask].detach()
            motion_vector_max[fired_layer2_mask] = motion_vector_layer1[fired_layer2_mask].detach()
            loser_pattern_index = torch.where(torch.logical_and(max_motion != 0, max_motion_layer1 != max_motion))
            fired2_index_x = loser_pattern_index[0]
            fired2_index_y = loser_pattern_index[1]
            voltage_block = max_v[None, None, :, :]
            voltage_block = F.pad(voltage_block, (self.padding_width, self.padding_width, self.padding_width, self.padding_width),
                                  mode='constant', value=0)
            voltage_block = F.unfold(voltage_block, (self.inh_size, self.inh_size))
            voltage_block = voltage_block.reshape([1, self.inh_size*self.inh_size, self.dvs_h, self.dvs_w])
            offset_pattern = torch.argmax(voltage_block, dim=1)
            offset_pattern = torch.squeeze(offset_pattern)
            offset_pattern_loser = offset_pattern[fired2_index_x, fired2_index_y]
            offset_x = offset_pattern_loser / self.inh_size - self.padding_width
            offset_y = torch.fmod(offset_pattern_loser, self.inh_size) - self.padding_width
            offset_x = offset_x.int()
            offset_y = offset_y.int()
            motion_vector_max[fired2_index_x, fired2_index_y, :] = motion_vector_max[fired2_index_x + offset_x,
                                                                                     fired2_index_y + offset_y, :]

        # 2. replace the loser motion pattern

        if visualize is True:
            Image_layer1 = torch.zeros(3, self.dvs_h, self.dvs_w).to(self.device)
            # ðŸš€ GPUä¼˜åŒ–ï¼šä½¿ç”¨ç¨ å¯†è®¿é—®æ›¿ä»£ç¨€ç–ç´¢å¼•ï¼Œtmp_motion_fullçŽ°åœ¨æ˜¯GPU tensor
            # å¯¹æ¯ä¸ªé¢œè‰²é€šé“è¿›è¡Œç¨ å¯†èµ‹å€¼
            for c in range(3):
                color_values = self.cc_motion[c, tmp_motion_full]  # èŽ·å–å¯¹åº”æ–¹å‘çš„é¢œè‰²å€¼
                Image_layer1[c] = torch.where(fired_spk, color_values, Image_layer1[c])

            Image_layer2 = torch.zeros(3, self.dvs_h, self.dvs_w).to(self.device)
            # å¯¹ç¬¬äºŒå±‚ä¹Ÿè¿›è¡Œç±»ä¼¼ä¼˜åŒ–ï¼Œä½†è¿™é‡Œtmp_vidæ˜¯CPUæ•°ç»„ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
            if has_layer2:
                for c in range(3):
                    color_values = torch.zeros(self.dvs_h, self.dvs_w, device=self.device)
                    color_values[fired_layer2_mask] = self.cc_motion[c, tmp_vid_tensor]
                    Image_layer2[c] = torch.where(fired_layer2_mask, color_values, Image_layer2[c])

            self.logger.add_image('motion_estimation/M1 estimation', Image_layer1, timestamp)
            self.logger.add_image('motion_estimation/MC estimation', Image_layer2, timestamp)

        # track_voltage.to(self.device_cpu)

        del dx, dy, tmp_motion, tmp_motion_full
        # Optional periodic empty_cache to mitigate allocator stalls in debug/replay; disabled by default
        if self.empty_cache_every > 0:
            self._call_step += 1
            if (self._call_step % self.empty_cache_every) == 0:
                torch.cuda.empty_cache()
 
        return max_motion, motion_vector_max, motion_vector_layer1
